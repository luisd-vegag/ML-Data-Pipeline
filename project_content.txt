File: main_dag.py

Path: /home/lv/Documents/Projects/ML-Data-Pipeline/dags/main_dag.py

```

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python_operator import PythonOperator

default_args = {
    "owner": "data_engineer",
    "depends_on_past": False,
    "start_date": datetime(2023, 4, 30),
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

dag = DAG(
    "main_dag",
    default_args=default_args,
    description="ETL pipeline for stock market dataset",
    schedule_interval=timedelta(days=1),
)

download_data_task = PythonOperator(
    task_id="download_data",
    python_callable=download_data,
    op_kwargs={
        "url": "https://www.kaggle.com/jacksoncrow/stock-market-dataset/download",
        "path": "data/raw/stock-market-dataset.zip",
    },
    dag=dag,
)

preprocess_data_task = PythonOperator(
    task_id="preprocess_data",
    python_callable=preprocess_data,
    op_kwargs={
        "input_path": "data/raw/Data/Stocks",
        "output_path": "data/processed/stocks.parquet",
    },
    dag=dag,
)

feature_engineering_task = PythonOperator(
    task_id="feature_engineering",
    python_callable=feature_engineering,
    op_kwargs={
        "input_path": "data/processed/stocks.parquet",
        "output_path": "data/processed/stocks_fe.parquet",
    },
    dag=dag,
)


download_data_task >> preprocess_data_task >> feature_engineering_task

```

File: stock_market_data_processing.py

Path: /home/lv/Documents/Projects/ML-Data-Pipeline/dags/stock_market_data_processing.py

```

import sys

sys.path.insert(0, "/home/lv/Documents/Projects/ML-Data-Pipeline/src")


from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python_operator import PythonOperator

from etl.download_data import download_data
from etl.preprocess_data import preprocess_data
from etl.feature_engineering import feature_engineering
from etl.train_model import train_model


default_args = {
    "owner": "your_name",
    "depends_on_past": False,
    "start_date": datetime(2023, 4, 30),
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

dag = DAG(
    "stock_market_data_processing",
    default_args=default_args,
    description="DAG for processing stock market data",
    schedule_interval=timedelta(days=1),
)

download_data_task = PythonOperator(
    task_id="download_data", python_callable=download_data, dag=dag
)

preprocess_data_task = PythonOperator(
    task_id="preprocess_data", python_callable=preprocess_data, dag=dag
)

feature_engineering_task = PythonOperator(
    task_id="feature_engineering", python_callable=feature_engineering, dag=dag
)

train_model_task = PythonOperator(
    task_id="train_model", python_callable=train_model, dag=dag
)

(
    download_data_task
    >> preprocess_data_task
    >> feature_engineering_task
    >> train_model_task
)

```

File: Dockerfile

Path: /home/lv/Documents/Projects/ML-Data-Pipeline/docker/Dockerfile

```

# Use an official Python runtime as a parent image
FROM python:3.9-slim

# Set the working directory to /app
WORKDIR /app

# Copy the current directory contents into the container at /app
COPY . /app

# Install any needed packages specified in requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Set the environment variable for Airflow home
ENV AIRFLOW_HOME /app

# Expose the Airflow webserver and scheduler ports
EXPOSE 8080 5555

# Start the Airflow webserver and scheduler
CMD ["airflow", "webserver", "--port", "8080"] 
CMD ["airflow", "scheduler"]

```

Directory: tests

File: test_feature_engineering.py

Path: /home/lv/Documents/Projects/ML-Data-Pipeline/src/tests/test_feature_engineering.py

```

import unittest
from feature_engineering import feature_engineering

class TestFeatureEngineering(unittest.TestCase):
    def test_feature_engineering(self):
        input_path = 'data/processed/stocks.parquet'
        output_path = 'data/processed/stocks_fe.parquet'
        feature_engineering(input_path, output_path)

        # TODO: Add assertions to verify the correctness of the feature engineered data


```

File: test_preprocess_data.py

Path: /home/lv/Documents/Projects/ML-Data-Pipeline/src/tests/test_preprocess_data.py

```

import unittest
from preprocess_data import preprocess_data

class TestPreprocessData(unittest.TestCase):
    def test_preprocess_data(self):
        input_path = 'data/raw/Data/Stocks'
        output_path = 'data/processed/stocks.parquet'
        preprocess_data(input_path, output_path)

        # TODO: Add assertions to verify the correctness of the processed data

```

Directory: etl

File: download_data.py

Path: /home/lv/Documents/Projects/ML-Data-Pipeline/src/etl/download_data.py

```

import os
import urllib.request

def download_data(url, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    urllib.request.urlretrieve(url, path)

if __name__ == '__main__':
    url = 'https://www.kaggle.com/jacksoncrow/stock-market-dataset/download'
    path = 'data/raw/stock-market-dataset.zip'
    download_data(url, path)

```

File: feature_engineering.py

Path: /home/lv/Documents/Projects/ML-Data-Pipeline/src/etl/feature_engineering.py

```

import os
import pandas as pd


def feature_engineering(input_path, output_path):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df = pd.read_parquet(input_path)
    # Calculate the rolling average of the trading volume for each stock and ETF
    df["vol_moving_avg"] = (
        df.groupby("Symbol")["Volume"]
        .rolling(window=30, min_periods=1)
        .mean()
        .reset_index(0, drop=True)
    )
    # Calculate the rolling median of the adjusted closing price for each stock and ETF
    df["adj_close_rolling_med"] = (
        df.groupby("Symbol")["Adj Close"]
        .rolling(window=30, min_periods=1)
        .median()
        .reset_index(0, drop=True)
    )
    # Save the feature engineered data to the same structured format
    df.to_parquet(output_path)


if __name__ == "__main__":
    input_path = "data/processed/"

```

File: train_model.py

Path: /home/lv/Documents/Projects/ML-Data-Pipeline/src/etl/train_model.py

```

import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error


def train_model(input_path, model_path):
    # Read the feature engineered data
    data = pd.read_parquet(input_path)

    # Prepare the input features and target variable
    X = data[["vol_moving_avg", "adj_close_rolling_med"]]
    y = data["Adj Close"]

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Train the model
    model = LinearRegression()
    model.fit(X_train, y_train)

    # Evaluate the model
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    print(f"Mean squared error: {mse:.2f}")

    # Save the trained model
    joblib.dump(model, model_path)


if __name__ == "__main__":
    input_path = "data/processed/stocks_fe.parquet"
    model_path = "models/linear_regression.pkl"
    train_model(input_path, model_path)

```

File: preprocess_data.py

Path: /home/lv/Documents/Projects/ML-Data-Pipeline/src/etl/preprocess_data.py

```

import os
import pandas as pd

def preprocess_data(input_path, output_path):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df = pd.read_csv(input_path)
    # Rename columns
    df = df.rename(columns={'tic': 'Symbol', 'datadate': 'Date', 'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'adjcp': 'Adj Close', 'volume': 'Volume', 'name': 'Security Name'})
    # Convert date column to datetime
    df['Date'] = pd.to_datetime(df['Date'], format='%Y%m%d')
    # Save the preprocessed data to a structured format
    df.to_parquet(output_path)

if __name__ == '__main__':
    input_path = 'data/raw/Data/Stocks'
    output_path = 'data/processed/stocks.parquet'
    preprocess_data(input_path, output_path)

```

File: log_config.py

Path: /home/lv/Documents/Projects/ML-Data-Pipeline/src/etl/log_config.py

```

import logging
import sys

logging.basicConfig(stream=sys.stdout, level=logging.INFO)

```

File: __init__.py

Path: /home/lv/Documents/Projects/ML-Data-Pipeline/src/etl/__init__.py

```


```


Directory: api

File: app.py

Path: /home/lv/Documents/Projects/ML-Data-Pipeline/src/api/app.py

```

from fastapi import FastAPI
from pydantic import BaseModel
import joblib
import pandas as pd

app = FastAPI()

class StockMarketData(BaseModel):
    vol_moving_avg: float
    adj_close_rolling_med: float

@app.post('/predict')
def predict(data: StockMarketData):
    # Load the trained model
    model = joblib.load('path/to/trained/model.pkl')

    # Create a Pandas DataFrame from the input data
    input_data = pd.DataFrame([{
        'vol_moving_avg': data.vol_moving_avg,
        'adj_close_rolling_med': data.adj_close_rolling_med
    }])

    # Make predictions using the trained model
    predictions = model.predict(input_data)

    # Return the predicted value
    return int(predictions[0])

```


File: __init__.py

Path: /home/lv/Documents/Projects/ML-Data-Pipeline/src/__init__.py

```


```

File: __init__.py

Path: /home/lv/Documents/Projects/ML-Data-Pipeline/src/__init__.py

